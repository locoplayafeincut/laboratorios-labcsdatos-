{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b5c0d2440b3e4995a794ded565213150",
        "deepnote_cell_type": "markdown",
        "id": "_Mql1uRoI5v5"
      },
      "source": [
        "<h1><center>Laboratorio 6: Optimizaci칩n de modelos 游빍</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci칩n Cient칤fica para Ciencia de Datos - Oto침o 2025</strong></center>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bfb94b9656f145ad83e81b75d218cb70",
        "deepnote_cell_type": "markdown",
        "id": "FAPGIlEAI5v8"
      },
      "source": [
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Stefano Schiappacasse, Sebasti치n Tinoco\n",
        "- Auxiliares: Melanie Pe침a, Valentina Rojas\n",
        "- Ayudantes: Angelo Mu침oz, Valentina Z칰침iga"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b1b537fdd27c43909a49d3476ce64d91",
        "deepnote_cell_type": "markdown",
        "id": "8NozgbkZI5v9"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser치n revisados\n",
        "\n",
        "- Nombre de alumno 1: Pablo Vergara\n",
        "- Nombre de alumno 2: Sasha Oyanadel\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e8qWRbJkcwP9"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/...../)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b7dbdd30ab544cb8a8afe00648a586ae",
        "deepnote_cell_type": "markdown",
        "id": "vHU9DI6wI5v9"
      },
      "source": [
        "### Temas a tratar\n",
        "\n",
        "- Predicci칩n de demanda usando `xgboost`\n",
        "- B칰squeda del modelo 칩ptimo de clasificaci칩n usando `optuna`\n",
        "- Uso de pipelines.\n",
        "\n",
        "\n",
        "### Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: 6 d칤as de plazo con descuento de 1 punto por d칤a. Entregas Martes a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda fuertemente asistir.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser치 debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est칠n en u-cursos no ser치n revisados. Recuerden que el repositorio tambi칠n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser치n respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "El laboratorio deber치 ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m치ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m치s eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f1c73babb7f74af588a4fa6ae14829e0",
        "deepnote_cell_type": "markdown",
        "id": "U_-sNOuOI5v9"
      },
      "source": [
        "# Importamos librerias 칰tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cell_id": "51afe4d2df42442b9e5402ffece60ead",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 4957,
        "execution_start": 1699544354044,
        "id": "ekHbM85NI5v9",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "!pip install -qq xgboost optuna"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y6hJXpLCSspz"
      },
      "source": [
        "# El emprendimiento de Fiu"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "44d227389a734ac59189c5e0005bc68a",
        "deepnote_cell_type": "markdown",
        "id": "b0bDalAOI5v-"
      },
      "source": [
        "Tras liderar de manera exitosa la implementaci칩n de un proyecto de ciencia de datos para caracterizar los datos generados en Santiago 2023, el misterioso corp칩reo **Fiu** se anima y decide levantar su propio negocio de consultor칤a en machine learning. Tras varias e intensas negociaciones, Fiu logra encontrar su *primera chamba*: predecir la demanda (cantidad de venta) de una famosa productora de bebidas de calibre mundial. Al ver el gran potencial y talento que usted ha demostrado en el campo de la ciencia de datos, Fiu lo contrata como data scientist para que forme parte de su nuevo emprendimiento.\n",
        "\n",
        "Para este laboratorio deben trabajar con los datos `sales.csv` subidos a u-cursos, el cual contiene una muestra de ventas de la empresa para diferentes productos en un determinado tiempo.\n",
        "\n",
        "Para comenzar, cargue el dataset se침alado y visualice a trav칠s de un `.head` los atributos que posee el dataset.\n",
        "\n",
        "<i><p align=\"center\">Fiu siendo felicitado por su excelente desempe침o en el proyecto de caracterizaci칩n de datos</p></i>\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media-front.elmostrador.cl/2023/09/A_UNO_1506411_2440e.jpg\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cell_id": "2f9c82d204b14515ad27ae07e0b77702",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 92,
        "execution_start": 1699544359006,
        "id": "QvMPOqHuI5v-",
        "source_hash": null
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>city</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>pop</th>\n",
              "      <th>shop</th>\n",
              "      <th>brand</th>\n",
              "      <th>container</th>\n",
              "      <th>capacity</th>\n",
              "      <th>price</th>\n",
              "      <th>quantity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2012-01-31</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>kinder-cola</td>\n",
              "      <td>glass</td>\n",
              "      <td>500ml</td>\n",
              "      <td>0.96</td>\n",
              "      <td>13280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2012-01-31</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>kinder-cola</td>\n",
              "      <td>plastic</td>\n",
              "      <td>1.5lt</td>\n",
              "      <td>2.86</td>\n",
              "      <td>6727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2012-01-31</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>kinder-cola</td>\n",
              "      <td>can</td>\n",
              "      <td>330ml</td>\n",
              "      <td>0.87</td>\n",
              "      <td>9848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2012-01-31</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>adult-cola</td>\n",
              "      <td>glass</td>\n",
              "      <td>500ml</td>\n",
              "      <td>1.00</td>\n",
              "      <td>20050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2012-01-31</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>adult-cola</td>\n",
              "      <td>can</td>\n",
              "      <td>330ml</td>\n",
              "      <td>0.39</td>\n",
              "      <td>25696</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id       date    city       lat      long     pop    shop        brand  \\\n",
              "0   0 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n",
              "1   1 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n",
              "2   2 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n",
              "3   3 2012-01-31  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n",
              "4   4 2012-01-31  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n",
              "\n",
              "  container capacity  price  quantity  \n",
              "0     glass    500ml   0.96     13280  \n",
              "1   plastic    1.5lt   2.86      6727  \n",
              "2       can    330ml   0.87      9848  \n",
              "3     glass    500ml   1.00     20050  \n",
              "4       can    330ml   0.39     25696  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "file_path = 'sales.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df['date'] = df['date'].apply(lambda x: datetime.strptime(x, \"%d/%m/%y\"))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b50db6f2cb804932ae3f9e5748a6ea61",
        "deepnote_cell_type": "markdown",
        "id": "pk4ru76pI5v_"
      },
      "source": [
        "## 1 Generando un Baseline (5 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/O-lan6TkadUAAAAC/what-i-wnna-do-after-a-baseline.gif\">\n",
        "</p>\n",
        "\n",
        "Antes de entrenar un algoritmo, usted recuerda los apuntes de su mag칤ster en ciencia de datos y recuerda que debe seguir una serie de *buenas pr치cticas* para entrenar correcta y debidamente su modelo. Despu칠s de un par de vueltas, llega a las siguientes tareas:\n",
        "\n",
        "1. Separe los datos en conjuntos de train (70%), validation (20%) y test (10%). Fije una semilla para controlar la aleatoriedad. [0.5 puntos]\n",
        "2. Implemente un `FunctionTransformer` para extraer el d칤a, mes y a침o de la variable `date`. Guarde estas variables en el formato categorical de pandas. [1 punto]\n",
        "3. Implemente un `ColumnTransformer` para procesar de manera adecuada los datos num칠ricos y categ칩ricos. Use `OneHotEncoder` para las variables categ칩ricas. `Nota:` Utilice el m칠todo `.set_output(transform='pandas')` para obtener un DataFrame como salida del `ColumnTransformer` [1 punto]\n",
        "4. Guarde los pasos anteriores en un `Pipeline`, dejando como 칰ltimo paso el regresor `DummyRegressor` para generar predicciones en base a promedios. [0.5 punto]\n",
        "5. Entrene el pipeline anterior y reporte la m칠trica `mean_absolute_error` sobre los datos de validaci칩n. 쮺칩mo se interpreta esta m칠trica para el contexto del negocio? [0.5 puntos]\n",
        "6. Finalmente, vuelva a entrenar el `Pipeline` pero esta vez usando `XGBRegressor` como modelo **utilizando los par치metros por default**. 쮺칩mo cambia el MAE al implementar este algoritmo? 쮼s mejor o peor que el `DummyRegressor`? [1 punto]\n",
        "7. Guarde ambos modelos en un archivo .pkl (uno cada uno) [0.5 puntos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cell_id": "1482c992d9494e5582b23dbd3431dbfd",
        "deepnote_cell_type": "code",
        "id": "sfnN7HubI5v_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE DummyRegressor: 13395.95\n",
            "MAE XGBRegressor: 2425.82\n",
            "XGBRegressor mejora el desempe침o respecto al baseline.\n"
          ]
        }
      ],
      "source": [
        "from sklearn import set_config\n",
        "set_config(transform_output=\"pandas\")\n",
        "\n",
        "# Inserte su c칩digo ac치\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "import pickle\n",
        "\n",
        "set_config(transform_output=\"pandas\")\n",
        "\n",
        "#1\n",
        "y = df[\"quantity\"]\n",
        "X = df.drop(columns=[\"quantity\", \"id\"])\n",
        "\n",
        "RANDOM_STATE = 666\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=RANDOM_STATE)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=2/9, random_state=RANDOM_STATE)\n",
        "\n",
        "#2\n",
        "def extract_date_features(df):\n",
        "    df = df.copy()\n",
        "    df[\"year\"] = df[\"date\"].dt.year.astype(\"category\")\n",
        "    df[\"month\"] = df[\"date\"].dt.month.astype(\"category\")\n",
        "    df[\"day\"] = df[\"date\"].dt.day.astype(\"category\")\n",
        "    return df.drop(columns=[\"date\"])\n",
        "\n",
        "date_transformer = FunctionTransformer(extract_date_features)\n",
        "\n",
        "#3\n",
        "X_train_transformed = extract_date_features(X_train)\n",
        "\n",
        "categorical_cols = X_train_transformed.select_dtypes(include=[\"category\", \"object\"]).columns.tolist()\n",
        "numerical_cols = X_train_transformed.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols),\n",
        "    (\"num\", StandardScaler(), numerical_cols)\n",
        "])\n",
        "\n",
        "#4\n",
        "pipeline_dummy = Pipeline(steps=[\n",
        "    (\"date_features\", date_transformer),\n",
        "    (\"preprocessing\", preprocessor),\n",
        "    (\"regressor\", DummyRegressor(strategy=\"mean\"))\n",
        "])\n",
        "\n",
        "#5\n",
        "pipeline_dummy.fit(X_train, y_train)\n",
        "y_val_pred_dummy = pipeline_dummy.predict(X_val)\n",
        "\n",
        "mae_dummy = mean_absolute_error(y_val, y_val_pred_dummy)\n",
        "print(f\"MAE DummyRegressor: {mae_dummy:.2f}\")\n",
        "\n",
        "#6\n",
        "pipeline_xgb = Pipeline(steps=[\n",
        "    (\"date_features\", date_transformer),\n",
        "    (\"preprocessing\", preprocessor),\n",
        "    (\"regressor\", XGBRegressor(random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "pipeline_xgb.fit(X_train, y_train)\n",
        "y_val_pred_xgb = pipeline_xgb.predict(X_val)\n",
        "\n",
        "mae_xgb = mean_absolute_error(y_val, y_val_pred_xgb)\n",
        "print(f\"MAE XGBRegressor: {mae_xgb:.2f}\")\n",
        "\n",
        "if mae_xgb < mae_dummy:\n",
        "    print(\"XGBRegressor mejora el desempe침o respecto al baseline.\")\n",
        "else:\n",
        "    print(\"XGBRegressor no mejora respecto al DummyRegressor.\")\n",
        "\n",
        "#7\n",
        "with open(\"modelo_dummy.pkl\", \"wb\") as f:\n",
        "    pickle.dump(pipeline_dummy, f)\n",
        "\n",
        "with open(\"modelo_xgb.pkl\", \"wb\") as f:\n",
        "    pickle.dump(pipeline_xgb, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    쮺칩mo se interpreta esta m칠trica para el contexto del negocio?\n",
        "El mean_absolute_error obtenido con el DummyRegressor fue de 13,395.95, lo que significa que, en promedio, el modelo se equivoca por m치s de 13 mil unidades al predecir la demanda. En el contexto del negocio, esto representa una predicci칩n extremadamente imprecisa, ya que el modelo no est치 utilizando ninguna informaci칩n de los datos m치s all치 del promedio hist칩rico. Este resultado es 칰til como l칤nea base m칤nima, pero inaceptable para decisiones comerciales reales.\n",
        "\n",
        "    쮺칩mo cambia el MAE al implementar este algoritmo? 쮼s mejor o peor que el DummyRegressor?\n",
        "El MAE baj칩 considerablemente al usar XGBRegressor, alcanzando un valor de 2425.82. Esto representa una mejora significativa respecto al DummyRegressor, que tuvo un MAE de 13,395.95. Por lo tanto, el modelo XGBoost es mucho mejor, ya que demuestra que puede capturar relaciones complejas entre las variables y predecir con mucha mayor precisi칩n la cantidad de ventas.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7e17e46063774ec28226fe300d42ffe0",
        "deepnote_cell_type": "markdown",
        "id": "wnyMINdKI5v_"
      },
      "source": [
        "## 2. Forzando relaciones entre par치metros con XGBoost (10 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://64.media.tumblr.com/14cc45f9610a6ee341a45fd0d68f4dde/20d11b36022bca7b-bf/s640x960/67ab1db12ff73a530f649ac455c000945d99c0d6.gif\">\n",
        "</p>\n",
        "\n",
        "Un colega aficionado a la econom칤a le *sopla* que la demanda guarda una relaci칩n inversa con el precio del producto. Motivado para impresionar al querido corp칩reo, se propone hacer uso de esta informaci칩n para mejorar su modelo realizando las siguientes tareas:\n",
        "\n",
        "1. Vuelva a entrenar el `Pipeline` con `XGBRegressor`, pero esta vez forzando una relaci칩n mon칩tona negativa entre el precio y la cantidad. Para aplicar esta restricci칩n ap칩yese en la siguiente <a href = https://xgboost.readthedocs.io/en/stable/tutorials/monotonic.html>documentaci칩n</a>. [6 puntos]\n",
        "\n",
        ">Hint 1: Para implementar el constraint se le sugiere hacerlo especificando el nombre de la variable. De ser as칤, probablemente le sea 칰til **mantener el formato de pandas** antes del step de entrenamiento.\n",
        "\n",
        ">Hint 2: Puede obtener el nombre de las columnas en el paso anterior al modelo regresor mediante el m칠todo `.get_feature_names_out()`\n",
        "\n",
        "2. Luego, vuelva a reportar el `MAE` sobre el conjunto de validaci칩n. [1 puntos]\n",
        "\n",
        "3. 쮺칩mo cambia el error al incluir esta relaci칩n? 쯊en칤a raz칩n su amigo? [2 puntos]\n",
        "\n",
        "4. Guarde su modelo en un archivo .pkl [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cell_id": "f469f3b572be434191d2d5c3f11b20d2",
        "deepnote_cell_type": "code",
        "id": "B7tMnkiAI5v_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE XGB con restricci칩n mon칩tona: 2454.15\n",
            "MAE sin restricci칩n: 2425.82\n",
            "MAE con restricci칩n: 2454.15\n",
            "Incluir la restricci칩n empeor칩 el rendimiento\n"
          ]
        }
      ],
      "source": [
        "# Inserte su c칩digo ac치\n",
        "#1\n",
        "X_train_transformed = extract_date_features(X_train)\n",
        "X_val_transformed = extract_date_features(X_val)\n",
        "\n",
        "preprocessor.fit(X_train_transformed)\n",
        "X_train_ready = preprocessor.transform(X_train_transformed)\n",
        "X_val_ready = preprocessor.transform(X_val_transformed)\n",
        "\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "monotonic_constraints = dict(\n",
        "    zip(\n",
        "        feature_names,\n",
        "        map(lambda name: -1 if \"price\" in name else 0, feature_names)\n",
        "    )\n",
        ")\n",
        "\n",
        "xgb_mono = XGBRegressor(\n",
        "    monotone_constraints=monotonic_constraints,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "xgb_mono.fit(X_train_ready, y_train)\n",
        "\n",
        "#2\n",
        "y_val_pred_mono = xgb_mono.predict(X_val_ready)\n",
        "mae_mono = mean_absolute_error(y_val, y_val_pred_mono)\n",
        "print(f\"MAE XGB con restricci칩n mon칩tona: {mae_mono:.2f}\")\n",
        "\n",
        "#3\n",
        "print(f\"MAE sin restricci칩n: {mae_xgb:.2f}\")\n",
        "print(f\"MAE con restricci칩n: {mae_mono:.2f}\")\n",
        "\n",
        "if mae_mono < mae_xgb:\n",
        "    print(\"Incluir la restricci칩n ayud칩 a mejorar el modelo.\")\n",
        "elif mae_mono == mae_xgb:\n",
        "    print(\"La restricci칩n no tuvo impacto en el rendimiento.\")\n",
        "else:\n",
        "    print(\"Incluir la restricci칩n empeor칩 el rendimiento\")\n",
        "\n",
        "#4\n",
        "with open(\"modelo_xgb_monotono.pkl\", \"wb\") as f:\n",
        "    pickle.dump(xgb_mono, f)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    쮺칩mo cambia el error al incluir esta relaci칩n? 쯊en칤a raz칩n su amigo?\n",
        "Incluir la restricci칩n mon칩tona negativa entre el precio y la cantidad no mejor칩 el desempe침o del modelo: el MAE aument칩 de 2425.82 (sin restricci칩n) a 2454.15 (con restricci칩n). Aunque el colega ten칤a una intuici칩n razonable al sugerir que el precio afecta negativamente la demanda, una idea v치lida desde la econom칤a, en este caso el modelo sin restricciones ya capturaba esa relaci칩n de forma natural. Forzarla termin칩 reduciendo la flexibilidad del modelo para aprender patrones m치s complejos, lo que afect칩 su precisi칩n. Por lo tanto, la sugerencia del colega, aunque bien intencionada, no result칩 beneficiosa en la pr치ctica."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e59ef80ed20b4de8921f24da74e87374",
        "deepnote_cell_type": "markdown",
        "id": "5D5-tX4dI5v_"
      },
      "source": [
        "## 1.3 Optimizaci칩n de Hiperpar치metros con Optuna (20 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/fmNdyGN4z5kAAAAi/hacking-lucy.gif\">\n",
        "</p>\n",
        "\n",
        "Luego de presentarle sus resultados, Fiu le pregunta si es posible mejorar *aun m치s* su modelo. En particular, le comenta de la optimizaci칩n de hiperpar치metros con metodolog칤as bayesianas a trav칠s del paquete `optuna`. Como usted es un aficionado al entrenamiento de modelos de ML, se propone implementar la descabellada idea de su jefe.\n",
        "\n",
        "A partir de la mejor configuraci칩n obtenida en la secci칩n anterior, utilice `optuna` para optimizar sus hiperpar치metros. En particular, se pide que su optimizaci칩n considere lo siguiente:\n",
        "\n",
        "- Fijar una semilla en las instancias necesarias para garantizar la reproducibilidad de resultados\n",
        "- Utilice `TPESampler` como m칠todo de muestreo\n",
        "- De `XGBRegressor`, optimice los siguientes hiperpar치metros:\n",
        "    - `learning_rate` buscando valores flotantes en el rango (0.001, 0.1)\n",
        "    - `n_estimators` buscando valores enteros en el rango (50, 1000)\n",
        "    - `max_depth` buscando valores enteros en el rango (3, 10)\n",
        "    - `max_leaves` buscando valores enteros en el rango (0, 100)\n",
        "    - `min_child_weight` buscando valores enteros en el rango (1, 5)\n",
        "    - `reg_alpha` buscando valores flotantes en el rango (0, 1)\n",
        "    - `reg_lambda` buscando valores flotantes en el rango (0, 1)\n",
        "- De `OneHotEncoder`, optimice el hiperpar치metro `min_frequency` buscando el mejor valor flotante en el rango (0.0, 1.0)\n",
        "\n",
        "Para ello se pide los siguientes pasos:\n",
        "1. Implemente una funci칩n `objective()` que permita minimizar el `MAE` en el conjunto de validaci칩n. Use el m칠todo `.set_user_attr()` para almacenar el mejor pipeline entrenado. [10 puntos]\n",
        "2. Fije el tiempo de entrenamiento a 5 minutos. [1 punto]\n",
        "3. Optimizar el modelo y reportar el n칰mero de *trials*, el `MAE` y los mejores hiperpar치metros encontrados. 쮺칩mo cambian sus resultados con respecto a la secci칩n anterior? 쮸 qu칠 se puede deber esto? [3 puntos]\n",
        "4. Explique cada hiperpar치metro y su rol en el modelo. 쮿acen sentido los rangos de optimizaci칩n indicados? [5 puntos]\n",
        "5. Guardar su modelo en un archivo .pkl [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cell_id": "de5914621cc64cb0b1bacb9ff565a97e",
        "deepnote_cell_type": "code",
        "id": "kMXXi1ckI5v_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trials realizados: 159\n",
            "Mejor MAE obtenido: 1999.37\n",
            "Mejores hiperpar치metros encontrados:\n",
            " - min_frequency: 0.020176029239194287\n",
            " - learning_rate: 0.05882575533214119\n",
            " - n_estimators: 999\n",
            " - max_depth: 9\n",
            " - max_leaves: 96\n",
            " - min_child_weight: 4\n",
            " - reg_alpha: 0.03076838107592547\n",
            " - reg_lambda: 0.029840863080500185\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "# Inserte su c칩digo ac치\n",
        "X_train_transformed = extract_date_features(X_train)\n",
        "X_val_transformed = extract_date_features(X_val)\n",
        "\n",
        "categorical_cols = X_train_transformed.select_dtypes(include=[\"category\", \"object\"]).columns.tolist()\n",
        "numerical_cols = X_train_transformed.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "def objective(trial):\n",
        "    # Inserte su c칩digo ac치\n",
        "    pass\n",
        "\n",
        "    min_freq = trial.suggest_float(\"min_frequency\", 0.0, 1.0)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.1)\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 1000)\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "    max_leaves = trial.suggest_int(\"max_leaves\", 0, 100)\n",
        "    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 5)\n",
        "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0.0, 1.0)\n",
        "    reg_lambda = trial.suggest_float(\"reg_lambda\", 0.0, 1.0)\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, min_frequency=min_freq), categorical_cols),\n",
        "        (\"num\", StandardScaler(), numerical_cols)\n",
        "    ])\n",
        "\n",
        "    pipeline = Pipeline(steps=[\n",
        "        (\"date_features\", date_transformer),\n",
        "        (\"preprocessing\", preprocessor),\n",
        "        (\"regressor\", XGBRegressor(\n",
        "            learning_rate=learning_rate,\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            max_leaves=max_leaves,\n",
        "            min_child_weight=min_child_weight,\n",
        "            reg_alpha=reg_alpha,\n",
        "            reg_lambda=reg_lambda,\n",
        "            random_state=RANDOM_STATE,\n",
        "            verbosity=0\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, y_pred)\n",
        "\n",
        "    trial.set_user_attr(\"best_pipeline\", pipeline)\n",
        "\n",
        "    return mae\n",
        "\n",
        "# 2\n",
        "sampler = TPESampler(seed=RANDOM_STATE)\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
        "\n",
        "study.optimize(objective, timeout=300) \n",
        "\n",
        "# 3\n",
        "print(f\"Trials realizados: {len(study.trials)}\")\n",
        "print(f\"Mejor MAE obtenido: {study.best_value:.2f}\")\n",
        "print(\"Mejores hiperpar치metros encontrados:\")\n",
        "for k, v in study.best_params.items():\n",
        "    print(f\" - {k}: {v}\")\n",
        "\n",
        "# 4\n",
        "best_model = study.best_trial.user_attrs[\"best_pipeline\"]\n",
        "\n",
        "with open(\"modelo_xgb_optuna.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_model, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    쮺칩mo cambian sus resultados con respecto a la secci칩n anterior? 쮸 qu칠 se puede deber esto?\n",
        "El mejor modelo obtenido con Optuna logr칩 un MAE de 1999.37, mejorando significativamente el resultado anterior con XGBRegressor por defecto, que alcanz칩 un MAE de 2425.82. Esta mejora de aproximadamente 17.6% en el error absoluto medio sugiere que la configuraci칩n predeterminada del modelo no era 칩ptima para este conjunto de datos. La optimizaci칩n permiti칩 encontrar una combinaci칩n m치s adecuada de par치metros que equilibra la profundidad, regularizaci칩n, tasa de aprendizaje y complejidad del modelo, lo cual mejora su capacidad de generalizar patrones y ajustarse a las caracter칤sticas espec칤ficas del problema. Adem치s, ajustar min_frequency en OneHotEncoder ayud칩 a reducir ruido de categor칤as poco frecuentes, mejorando a칰n m치s la robustez del modelo.\n",
        "\n",
        "    Explique cada hiperpar치metro y su rol en el modelo.\n",
        "* learning_rate: controla qu칠 tan r치pido aprende el modelo. Un valor moderado como 0.058 permite ajustes graduales, evitando sobreajuste. El rango (0.001, 0.1) es razonable para evitar valores extremos.\n",
        "* n_estimators: define la cantidad de 치rboles. Un valor alto como 999 es coherente con un learning_rate bajo, ya que se necesita m치s iteraciones para converger. El rango (50, 1000) es apropiado.\n",
        "* max_depth y max_leaves: controlan la complejidad del 치rbol. Valores altos como 9 y 96 permiten al modelo capturar interacciones complejas. Los rangos dados son l칩gicos para evitar tanto modelos muy simples como sobreajuste extremo.\n",
        "* min_child_weight: evita que nodos con poca informaci칩n sean divididos. Un valor de 4 reduce el riesgo de overfitting. El rango (1, 5) es adecuado.\n",
        "* reg_alpha y reg_lambda: controlan la regularizaci칩n L1 y L2 respectivamente. Ambos ayudan a prevenir el sobreajuste. Los valores encontrados (~0.03) sugieren que cierta penalizaci칩n mejora la generalizaci칩n. El rango (0, 1) es est치ndar y apropiado.\n",
        "* min_frequency en OneHotEncoder: filtra categor칤as poco frecuentes. Un valor bajo (0.02) permite mantener la mayor parte de la informaci칩n sin introducir ruido. El rango (0.0, 1.0) ofrece flexibilidad para distintos niveles de cardinalidad.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    쮿acen sentido los rangos de optimizaci칩n indicados?\n",
        "En conjunto, los rangos de optimizaci칩n son adecuados, y los valores encontrados tienen sentido dado el problema y los resultados obtenidos."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5195ccfc37e044ad9453f6eb2754f631",
        "deepnote_cell_type": "markdown",
        "id": "ZglyD_QWI5wA"
      },
      "source": [
        "## 4. Optimizaci칩n de Hiperpar치metros con Optuna y Prunners (17 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.pinimg.com/originals/90/16/f9/9016f919c2259f3d0e8fe465049638a7.gif\">\n",
        "</p>\n",
        "\n",
        "Despu칠s de optimizar el rendimiento de su modelo varias veces, Fiu le pregunta si no es posible optimizar el entrenamiento del modelo en s칤 mismo. Despu칠s de leer un par de post de personas de dudosa reputaci칩n en la *deepweb*, usted llega a la conclusi칩n que puede cumplir este objetivo mediante la implementaci칩n de **Prunning**.\n",
        "\n",
        "Vuelva a optimizar los mismos hiperpar치metros que la secci칩n pasada, pero esta vez utilizando **Prunning** en la optimizaci칩n. En particular, usted debe:\n",
        "\n",
        "- Responder: 쯈u칠 es prunning? 쮻e qu칠 forma deber칤a impactar en el entrenamiento? [2 puntos]\n",
        "- Redefinir la funci칩n `objective()` utilizando `optuna.integration.XGBoostPruningCallback` como m칠todo de **Prunning** [10 puntos]\n",
        "- Fijar nuevamente el tiempo de entrenamiento a 5 minutos [1 punto]\n",
        "- Reportar el n칰mero de *trials*, el `MAE` y los mejores hiperpar치metros encontrados. 쮺칩mo cambian sus resultados con respecto a la secci칩n anterior? 쮸 qu칠 se puede deber esto? [3 puntos]\n",
        "- Guardar su modelo en un archivo .pkl [1 punto]\n",
        "\n",
        "Nota: Si quieren silenciar los prints obtenidos en el prunning, pueden hacerlo mediante el siguiente comando:\n",
        "\n",
        "```\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "```\n",
        "\n",
        "De implementar la opci칩n anterior, pueden especificar `show_progress_bar = True` en el m칠todo `optimize` para *m치s sabor*.\n",
        "\n",
        "Hint: Si quieren especificar par치metros del m칠todo .fit() del modelo a trav칠s del pipeline, pueden hacerlo por medio de la siguiente sintaxis: `pipeline.fit(stepmodelo__parametro = valor)`\n",
        "\n",
        "Hint2: Este <a href = https://stackoverflow.com/questions/40329576/sklearn-pass-fit-parameters-to-xgboost-in-pipeline>enlace</a> les puede ser de ayuda en su implementaci칩n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IEEZnb-cwP_"
      },
      "outputs": [],
      "source": [
        "#!pip install optuna-integration[xgboost]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "eeaa967cd8f6426d8c54f276c17dce79",
        "deepnote_cell_type": "code",
        "id": "sST6Wtj5I5wA"
      },
      "outputs": [],
      "source": [
        "# Inserte su c칩digo ac치"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8a081778cc704fc6bed05393a5419327",
        "deepnote_cell_type": "markdown",
        "id": "ZMiiVaCUI5wA"
      },
      "source": [
        "## 5. Visualizaciones (5 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/F-LgB1xTebEAAAAd/look-at-this-graph-nickelback.gif\">\n",
        "</p>\n",
        "\n",
        "\n",
        "Satisfecho con su trabajo, Fiu le pregunta si es posible generar visualizaciones que permitan entender el entrenamiento de su modelo.\n",
        "\n",
        "A partir del siguiente <a href = https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html#visualization>enlace</a>, genere las siguientes visualizaciones:\n",
        "\n",
        "1. Gr치fico de historial de optimizaci칩n [1 punto]\n",
        "2. Gr치fico de coordenadas paralelas [1 punto]\n",
        "3. Gr치fico de importancia de hiperpar치metros [1 punto]\n",
        "\n",
        "Comente sus resultados:\n",
        "\n",
        "4. 쮻esde qu칠 *trial* se empiezan a observar mejoras notables en sus resultados? [0.5 puntos]\n",
        "5. 쯈u칠 tendencias puede observar a partir del gr치fico de coordenadas paralelas? [1 punto]\n",
        "6. 쮺u치les son los hiperpar치metros con mayor importancia para la optimizaci칩n de su modelo? [0.5 puntos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "0e706dc9a8d946eda7a9eb1f0463c6d7",
        "deepnote_cell_type": "code",
        "id": "xjxAEENAI5wA"
      },
      "outputs": [],
      "source": [
        "# Inserte su c칩digo ac치"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ac8a20f445d045a3becf1a518d410a7d",
        "deepnote_cell_type": "markdown",
        "id": "EoW32TA9I5wA"
      },
      "source": [
        "## 6. S칤ntesis de resultados (3 puntos)\n",
        "\n",
        "Finalmente:\n",
        "\n",
        "1. Genere una tabla resumen del MAE en el conjunto de validaci칩n obtenido en los 5 modelos entrenados desde Baseline hasta XGBoost con Constraints, Optuna y Prunning. [1 punto]\n",
        "2. Compare los resultados de la tabla y responda, 쯤u칠 modelo obtiene el mejor rendimiento? [0.5 puntos]\n",
        "3. Cargue el mejor modelo, prediga sobre el conjunto de **test** y reporte su MAE. [0.5 puntos]\n",
        "4. 쮼xisten diferencias con respecto a las m칠tricas obtenidas en el conjunto de validaci칩n? 쯇orqu칠 puede ocurrir esto? [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq5C6cDnJg9h"
      },
      "outputs": [],
      "source": [
        "# Inserte su c칩digo ac치"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5c4654d12037494fbd385b4dc6bd1059",
        "deepnote_cell_type": "markdown",
        "id": "E_19tgBEI5wA"
      },
      "source": [
        "# Conclusi칩n\n",
        "Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.pinimg.com/originals/55/3d/42/553d42bea9b10e0662a05aa8726fc7f4.gif\">\n",
        "</p>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5025de06759f4903a26916c80323bf25",
        "deepnote_cell_type": "markdown",
        "id": "Kq2cFix1I5wA"
      },
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "rAp9UxwiI5wA"
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "f63d38450a6b464c9bb6385cf11db4d9",
    "deepnote_persisted_session": {
      "createdAt": "2023-11-09T16:18:30.203Z"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
