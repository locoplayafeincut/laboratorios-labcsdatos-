{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b5c0d2440b3e4995a794ded565213150",
        "deepnote_cell_type": "markdown",
        "id": "_Mql1uRoI5v5"
      },
      "source": [
        "<h1><center>Laboratorio 6: Optimización de modelos 🧪</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programación Científica para Ciencia de Datos - Otoño 2025</strong></center>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bfb94b9656f145ad83e81b75d218cb70",
        "deepnote_cell_type": "markdown",
        "id": "FAPGIlEAI5v8"
      },
      "source": [
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Stefano Schiappacasse, Sebastián Tinoco\n",
        "- Auxiliares: Melanie Peña, Valentina Rojas\n",
        "- Ayudantes: Angelo Muñoz, Valentina Zúñiga"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b1b537fdd27c43909a49d3476ce64d91",
        "deepnote_cell_type": "markdown",
        "id": "8NozgbkZI5v9"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados\n",
        "\n",
        "- Nombre de alumno 1: Pablo Vergara\n",
        "- Nombre de alumno 2: Sasha Oyanadel\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e8qWRbJkcwP9"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/...../)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b7dbdd30ab544cb8a8afe00648a586ae",
        "deepnote_cell_type": "markdown",
        "id": "vHU9DI6wI5v9"
      },
      "source": [
        "### Temas a tratar\n",
        "\n",
        "- Predicción de demanda usando `xgboost`\n",
        "- Búsqueda del modelo óptimo de clasificación usando `optuna`\n",
        "- Uso de pipelines.\n",
        "\n",
        "\n",
        "### Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: 6 días de plazo con descuento de 1 punto por día. Entregas Martes a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda fuertemente asistir.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia será debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no estén en u-cursos no serán revisados. Recuerden que el repositorio también tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f1c73babb7f74af588a4fa6ae14829e0",
        "deepnote_cell_type": "markdown",
        "id": "U_-sNOuOI5v9"
      },
      "source": [
        "# Importamos librerias útiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cell_id": "51afe4d2df42442b9e5402ffece60ead",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 4957,
        "execution_start": 1699544354044,
        "id": "ekHbM85NI5v9",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "!pip install -qq xgboost optuna"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y6hJXpLCSspz"
      },
      "source": [
        "# El emprendimiento de Fiu"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "44d227389a734ac59189c5e0005bc68a",
        "deepnote_cell_type": "markdown",
        "id": "b0bDalAOI5v-"
      },
      "source": [
        "Tras liderar de manera exitosa la implementación de un proyecto de ciencia de datos para caracterizar los datos generados en Santiago 2023, el misterioso corpóreo **Fiu** se anima y decide levantar su propio negocio de consultoría en machine learning. Tras varias e intensas negociaciones, Fiu logra encontrar su *primera chamba*: predecir la demanda (cantidad de venta) de una famosa productora de bebidas de calibre mundial. Al ver el gran potencial y talento que usted ha demostrado en el campo de la ciencia de datos, Fiu lo contrata como data scientist para que forme parte de su nuevo emprendimiento.\n",
        "\n",
        "Para este laboratorio deben trabajar con los datos `sales.csv` subidos a u-cursos, el cual contiene una muestra de ventas de la empresa para diferentes productos en un determinado tiempo.\n",
        "\n",
        "Para comenzar, cargue el dataset señalado y visualice a través de un `.head` los atributos que posee el dataset.\n",
        "\n",
        "<i><p align=\"center\">Fiu siendo felicitado por su excelente desempeño en el proyecto de caracterización de datos</p></i>\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media-front.elmostrador.cl/2023/09/A_UNO_1506411_2440e.jpg\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cell_id": "2f9c82d204b14515ad27ae07e0b77702",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 92,
        "execution_start": 1699544359006,
        "id": "QvMPOqHuI5v-",
        "source_hash": null
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>city</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>pop</th>\n",
              "      <th>shop</th>\n",
              "      <th>brand</th>\n",
              "      <th>container</th>\n",
              "      <th>capacity</th>\n",
              "      <th>price</th>\n",
              "      <th>quantity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2012-01-31</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>kinder-cola</td>\n",
              "      <td>glass</td>\n",
              "      <td>500ml</td>\n",
              "      <td>0.96</td>\n",
              "      <td>13280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2012-01-31</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>kinder-cola</td>\n",
              "      <td>plastic</td>\n",
              "      <td>1.5lt</td>\n",
              "      <td>2.86</td>\n",
              "      <td>6727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2012-01-31</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>kinder-cola</td>\n",
              "      <td>can</td>\n",
              "      <td>330ml</td>\n",
              "      <td>0.87</td>\n",
              "      <td>9848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2012-01-31</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>adult-cola</td>\n",
              "      <td>glass</td>\n",
              "      <td>500ml</td>\n",
              "      <td>1.00</td>\n",
              "      <td>20050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2012-01-31</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>adult-cola</td>\n",
              "      <td>can</td>\n",
              "      <td>330ml</td>\n",
              "      <td>0.39</td>\n",
              "      <td>25696</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id       date    city       lat      long     pop    shop        brand  \\\n",
              "0   0 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n",
              "1   1 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n",
              "2   2 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n",
              "3   3 2012-01-31  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n",
              "4   4 2012-01-31  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n",
              "\n",
              "  container capacity  price  quantity  \n",
              "0     glass    500ml   0.96     13280  \n",
              "1   plastic    1.5lt   2.86      6727  \n",
              "2       can    330ml   0.87      9848  \n",
              "3     glass    500ml   1.00     20050  \n",
              "4       can    330ml   0.39     25696  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "file_path = 'sales.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df['date'] = df['date'].apply(lambda x: datetime.strptime(x, \"%d/%m/%y\"))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b50db6f2cb804932ae3f9e5748a6ea61",
        "deepnote_cell_type": "markdown",
        "id": "pk4ru76pI5v_"
      },
      "source": [
        "## 1 Generando un Baseline (5 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/O-lan6TkadUAAAAC/what-i-wnna-do-after-a-baseline.gif\">\n",
        "</p>\n",
        "\n",
        "Antes de entrenar un algoritmo, usted recuerda los apuntes de su magíster en ciencia de datos y recuerda que debe seguir una serie de *buenas prácticas* para entrenar correcta y debidamente su modelo. Después de un par de vueltas, llega a las siguientes tareas:\n",
        "\n",
        "1. Separe los datos en conjuntos de train (70%), validation (20%) y test (10%). Fije una semilla para controlar la aleatoriedad. [0.5 puntos]\n",
        "2. Implemente un `FunctionTransformer` para extraer el día, mes y año de la variable `date`. Guarde estas variables en el formato categorical de pandas. [1 punto]\n",
        "3. Implemente un `ColumnTransformer` para procesar de manera adecuada los datos numéricos y categóricos. Use `OneHotEncoder` para las variables categóricas. `Nota:` Utilice el método `.set_output(transform='pandas')` para obtener un DataFrame como salida del `ColumnTransformer` [1 punto]\n",
        "4. Guarde los pasos anteriores en un `Pipeline`, dejando como último paso el regresor `DummyRegressor` para generar predicciones en base a promedios. [0.5 punto]\n",
        "5. Entrene el pipeline anterior y reporte la métrica `mean_absolute_error` sobre los datos de validación. ¿Cómo se interpreta esta métrica para el contexto del negocio? [0.5 puntos]\n",
        "6. Finalmente, vuelva a entrenar el `Pipeline` pero esta vez usando `XGBRegressor` como modelo **utilizando los parámetros por default**. ¿Cómo cambia el MAE al implementar este algoritmo? ¿Es mejor o peor que el `DummyRegressor`? [1 punto]\n",
        "7. Guarde ambos modelos en un archivo .pkl (uno cada uno) [0.5 puntos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cell_id": "1482c992d9494e5582b23dbd3431dbfd",
        "deepnote_cell_type": "code",
        "id": "sfnN7HubI5v_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE DummyRegressor: 13395.95\n",
            "MAE XGBRegressor: 2425.82\n",
            "XGBRegressor mejora el desempeño respecto al baseline.\n"
          ]
        }
      ],
      "source": [
        "from sklearn import set_config\n",
        "set_config(transform_output=\"pandas\")\n",
        "\n",
        "# Inserte su código acá\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "import pickle\n",
        "\n",
        "set_config(transform_output=\"pandas\")\n",
        "\n",
        "#1\n",
        "y = df[\"quantity\"]\n",
        "X = df.drop(columns=[\"quantity\", \"id\"])\n",
        "\n",
        "RANDOM_STATE = 666\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=RANDOM_STATE)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=2/9, random_state=RANDOM_STATE)\n",
        "\n",
        "#2\n",
        "def extract_date_features(df):\n",
        "    df = df.copy()\n",
        "    df[\"year\"] = df[\"date\"].dt.year.astype(\"category\")\n",
        "    df[\"month\"] = df[\"date\"].dt.month.astype(\"category\")\n",
        "    df[\"day\"] = df[\"date\"].dt.day.astype(\"category\")\n",
        "    return df.drop(columns=[\"date\"])\n",
        "\n",
        "date_transformer = FunctionTransformer(extract_date_features)\n",
        "\n",
        "#3\n",
        "X_train_transformed = extract_date_features(X_train)\n",
        "\n",
        "categorical_cols = X_train_transformed.select_dtypes(include=[\"category\", \"object\"]).columns.tolist()\n",
        "numerical_cols = X_train_transformed.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols),\n",
        "    (\"num\", StandardScaler(), numerical_cols)\n",
        "])\n",
        "\n",
        "#4\n",
        "pipeline_dummy = Pipeline(steps=[\n",
        "    (\"date_features\", date_transformer),\n",
        "    (\"preprocessing\", preprocessor),\n",
        "    (\"regressor\", DummyRegressor(strategy=\"mean\"))\n",
        "])\n",
        "\n",
        "#5\n",
        "pipeline_dummy.fit(X_train, y_train)\n",
        "y_val_pred_dummy = pipeline_dummy.predict(X_val)\n",
        "\n",
        "mae_dummy = mean_absolute_error(y_val, y_val_pred_dummy)\n",
        "print(f\"MAE DummyRegressor: {mae_dummy:.2f}\")\n",
        "\n",
        "#6\n",
        "pipeline_xgb = Pipeline(steps=[\n",
        "    (\"date_features\", date_transformer),\n",
        "    (\"preprocessing\", preprocessor),\n",
        "    (\"regressor\", XGBRegressor(random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "pipeline_xgb.fit(X_train, y_train)\n",
        "y_val_pred_xgb = pipeline_xgb.predict(X_val)\n",
        "\n",
        "mae_xgb = mean_absolute_error(y_val, y_val_pred_xgb)\n",
        "print(f\"MAE XGBRegressor: {mae_xgb:.2f}\")\n",
        "\n",
        "if mae_xgb < mae_dummy:\n",
        "    print(\"XGBRegressor mejora el desempeño respecto al baseline.\")\n",
        "else:\n",
        "    print(\"XGBRegressor no mejora respecto al DummyRegressor.\")\n",
        "\n",
        "#7\n",
        "with open(\"modelo_dummy.pkl\", \"wb\") as f:\n",
        "    pickle.dump(pipeline_dummy, f)\n",
        "\n",
        "with open(\"modelo_xgb.pkl\", \"wb\") as f:\n",
        "    pickle.dump(pipeline_xgb, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    ¿Cómo se interpreta esta métrica para el contexto del negocio?\n",
        "El mean_absolute_error obtenido con el DummyRegressor fue de 13,395.95, lo que significa que, en promedio, el modelo se equivoca por más de 13 mil unidades al predecir la demanda. En el contexto del negocio, esto representa una predicción extremadamente imprecisa, ya que el modelo no está utilizando ninguna información de los datos más allá del promedio histórico. Este resultado es útil como línea base mínima, pero inaceptable para decisiones comerciales reales.\n",
        "\n",
        "    ¿Cómo cambia el MAE al implementar este algoritmo? ¿Es mejor o peor que el DummyRegressor?\n",
        "El MAE bajó considerablemente al usar XGBRegressor, alcanzando un valor de 2425.82. Esto representa una mejora significativa respecto al DummyRegressor, que tuvo un MAE de 13,395.95. Por lo tanto, el modelo XGBoost es mucho mejor, ya que demuestra que puede capturar relaciones complejas entre las variables y predecir con mucha mayor precisión la cantidad de ventas.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7e17e46063774ec28226fe300d42ffe0",
        "deepnote_cell_type": "markdown",
        "id": "wnyMINdKI5v_"
      },
      "source": [
        "## 2. Forzando relaciones entre parámetros con XGBoost (10 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://64.media.tumblr.com/14cc45f9610a6ee341a45fd0d68f4dde/20d11b36022bca7b-bf/s640x960/67ab1db12ff73a530f649ac455c000945d99c0d6.gif\">\n",
        "</p>\n",
        "\n",
        "Un colega aficionado a la economía le *sopla* que la demanda guarda una relación inversa con el precio del producto. Motivado para impresionar al querido corpóreo, se propone hacer uso de esta información para mejorar su modelo realizando las siguientes tareas:\n",
        "\n",
        "1. Vuelva a entrenar el `Pipeline` con `XGBRegressor`, pero esta vez forzando una relación monótona negativa entre el precio y la cantidad. Para aplicar esta restricción apóyese en la siguiente <a href = https://xgboost.readthedocs.io/en/stable/tutorials/monotonic.html>documentación</a>. [6 puntos]\n",
        "\n",
        ">Hint 1: Para implementar el constraint se le sugiere hacerlo especificando el nombre de la variable. De ser así, probablemente le sea útil **mantener el formato de pandas** antes del step de entrenamiento.\n",
        "\n",
        ">Hint 2: Puede obtener el nombre de las columnas en el paso anterior al modelo regresor mediante el método `.get_feature_names_out()`\n",
        "\n",
        "2. Luego, vuelva a reportar el `MAE` sobre el conjunto de validación. [1 puntos]\n",
        "\n",
        "3. ¿Cómo cambia el error al incluir esta relación? ¿Tenía razón su amigo? [2 puntos]\n",
        "\n",
        "4. Guarde su modelo en un archivo .pkl [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cell_id": "f469f3b572be434191d2d5c3f11b20d2",
        "deepnote_cell_type": "code",
        "id": "B7tMnkiAI5v_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE XGB con restricción monótona: 2454.15\n",
            "MAE sin restricción: 2425.82\n",
            "MAE con restricción: 2454.15\n",
            "Incluir la restricción empeoró el rendimiento\n"
          ]
        }
      ],
      "source": [
        "# Inserte su código acá\n",
        "#1\n",
        "X_train_transformed = extract_date_features(X_train)\n",
        "X_val_transformed = extract_date_features(X_val)\n",
        "\n",
        "preprocessor.fit(X_train_transformed)\n",
        "X_train_ready = preprocessor.transform(X_train_transformed)\n",
        "X_val_ready = preprocessor.transform(X_val_transformed)\n",
        "\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "monotonic_constraints = dict(\n",
        "    zip(\n",
        "        feature_names,\n",
        "        map(lambda name: -1 if \"price\" in name else 0, feature_names)\n",
        "    )\n",
        ")\n",
        "\n",
        "xgb_mono = XGBRegressor(\n",
        "    monotone_constraints=monotonic_constraints,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "xgb_mono.fit(X_train_ready, y_train)\n",
        "\n",
        "#2\n",
        "y_val_pred_mono = xgb_mono.predict(X_val_ready)\n",
        "mae_mono = mean_absolute_error(y_val, y_val_pred_mono)\n",
        "print(f\"MAE XGB con restricción monótona: {mae_mono:.2f}\")\n",
        "\n",
        "#3\n",
        "print(f\"MAE sin restricción: {mae_xgb:.2f}\")\n",
        "print(f\"MAE con restricción: {mae_mono:.2f}\")\n",
        "\n",
        "if mae_mono < mae_xgb:\n",
        "    print(\"Incluir la restricción ayudó a mejorar el modelo.\")\n",
        "elif mae_mono == mae_xgb:\n",
        "    print(\"La restricción no tuvo impacto en el rendimiento.\")\n",
        "else:\n",
        "    print(\"Incluir la restricción empeoró el rendimiento\")\n",
        "\n",
        "#4\n",
        "with open(\"modelo_xgb_monotono.pkl\", \"wb\") as f:\n",
        "    pickle.dump(xgb_mono, f)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    ¿Cómo cambia el error al incluir esta relación? ¿Tenía razón su amigo?\n",
        "Incluir la restricción monótona negativa entre el precio y la cantidad no mejoró el desempeño del modelo: el MAE aumentó de 2425.82 (sin restricción) a 2454.15 (con restricción). Aunque el colega tenía una intuición razonable al sugerir que el precio afecta negativamente la demanda, una idea válida desde la economía, en este caso el modelo sin restricciones ya capturaba esa relación de forma natural. Forzarla terminó reduciendo la flexibilidad del modelo para aprender patrones más complejos, lo que afectó su precisión. Por lo tanto, la sugerencia del colega, aunque bien intencionada, no resultó beneficiosa en la práctica."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e59ef80ed20b4de8921f24da74e87374",
        "deepnote_cell_type": "markdown",
        "id": "5D5-tX4dI5v_"
      },
      "source": [
        "## 1.3 Optimización de Hiperparámetros con Optuna (20 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/fmNdyGN4z5kAAAAi/hacking-lucy.gif\">\n",
        "</p>\n",
        "\n",
        "Luego de presentarle sus resultados, Fiu le pregunta si es posible mejorar *aun más* su modelo. En particular, le comenta de la optimización de hiperparámetros con metodologías bayesianas a través del paquete `optuna`. Como usted es un aficionado al entrenamiento de modelos de ML, se propone implementar la descabellada idea de su jefe.\n",
        "\n",
        "A partir de la mejor configuración obtenida en la sección anterior, utilice `optuna` para optimizar sus hiperparámetros. En particular, se pide que su optimización considere lo siguiente:\n",
        "\n",
        "- Fijar una semilla en las instancias necesarias para garantizar la reproducibilidad de resultados\n",
        "- Utilice `TPESampler` como método de muestreo\n",
        "- De `XGBRegressor`, optimice los siguientes hiperparámetros:\n",
        "    - `learning_rate` buscando valores flotantes en el rango (0.001, 0.1)\n",
        "    - `n_estimators` buscando valores enteros en el rango (50, 1000)\n",
        "    - `max_depth` buscando valores enteros en el rango (3, 10)\n",
        "    - `max_leaves` buscando valores enteros en el rango (0, 100)\n",
        "    - `min_child_weight` buscando valores enteros en el rango (1, 5)\n",
        "    - `reg_alpha` buscando valores flotantes en el rango (0, 1)\n",
        "    - `reg_lambda` buscando valores flotantes en el rango (0, 1)\n",
        "- De `OneHotEncoder`, optimice el hiperparámetro `min_frequency` buscando el mejor valor flotante en el rango (0.0, 1.0)\n",
        "\n",
        "Para ello se pide los siguientes pasos:\n",
        "1. Implemente una función `objective()` que permita minimizar el `MAE` en el conjunto de validación. Use el método `.set_user_attr()` para almacenar el mejor pipeline entrenado. [10 puntos]\n",
        "2. Fije el tiempo de entrenamiento a 5 minutos. [1 punto]\n",
        "3. Optimizar el modelo y reportar el número de *trials*, el `MAE` y los mejores hiperparámetros encontrados. ¿Cómo cambian sus resultados con respecto a la sección anterior? ¿A qué se puede deber esto? [3 puntos]\n",
        "4. Explique cada hiperparámetro y su rol en el modelo. ¿Hacen sentido los rangos de optimización indicados? [5 puntos]\n",
        "5. Guardar su modelo en un archivo .pkl [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cell_id": "de5914621cc64cb0b1bacb9ff565a97e",
        "deepnote_cell_type": "code",
        "id": "kMXXi1ckI5v_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trials realizados: 159\n",
            "Mejor MAE obtenido: 1999.37\n",
            "Mejores hiperparámetros encontrados:\n",
            " - min_frequency: 0.020176029239194287\n",
            " - learning_rate: 0.05882575533214119\n",
            " - n_estimators: 999\n",
            " - max_depth: 9\n",
            " - max_leaves: 96\n",
            " - min_child_weight: 4\n",
            " - reg_alpha: 0.03076838107592547\n",
            " - reg_lambda: 0.029840863080500185\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "# Inserte su código acá\n",
        "X_train_transformed = extract_date_features(X_train)\n",
        "X_val_transformed = extract_date_features(X_val)\n",
        "\n",
        "categorical_cols = X_train_transformed.select_dtypes(include=[\"category\", \"object\"]).columns.tolist()\n",
        "numerical_cols = X_train_transformed.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "def objective(trial):\n",
        "    # Inserte su código acá\n",
        "    pass\n",
        "\n",
        "    min_freq = trial.suggest_float(\"min_frequency\", 0.0, 1.0)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.1)\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 1000)\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "    max_leaves = trial.suggest_int(\"max_leaves\", 0, 100)\n",
        "    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 5)\n",
        "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0.0, 1.0)\n",
        "    reg_lambda = trial.suggest_float(\"reg_lambda\", 0.0, 1.0)\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, min_frequency=min_freq), categorical_cols),\n",
        "        (\"num\", StandardScaler(), numerical_cols)\n",
        "    ])\n",
        "\n",
        "    pipeline = Pipeline(steps=[\n",
        "        (\"date_features\", date_transformer),\n",
        "        (\"preprocessing\", preprocessor),\n",
        "        (\"regressor\", XGBRegressor(\n",
        "            learning_rate=learning_rate,\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            max_leaves=max_leaves,\n",
        "            min_child_weight=min_child_weight,\n",
        "            reg_alpha=reg_alpha,\n",
        "            reg_lambda=reg_lambda,\n",
        "            random_state=RANDOM_STATE,\n",
        "            verbosity=0\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, y_pred)\n",
        "\n",
        "    trial.set_user_attr(\"best_pipeline\", pipeline)\n",
        "\n",
        "    return mae\n",
        "\n",
        "# 2\n",
        "sampler = TPESampler(seed=RANDOM_STATE)\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
        "\n",
        "study.optimize(objective, timeout=300) \n",
        "\n",
        "# 3\n",
        "print(f\"Trials realizados: {len(study.trials)}\")\n",
        "print(f\"Mejor MAE obtenido: {study.best_value:.2f}\")\n",
        "print(\"Mejores hiperparámetros encontrados:\")\n",
        "for k, v in study.best_params.items():\n",
        "    print(f\" - {k}: {v}\")\n",
        "\n",
        "# 4\n",
        "best_model = study.best_trial.user_attrs[\"best_pipeline\"]\n",
        "\n",
        "with open(\"modelo_xgb_optuna.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_model, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    ¿Cómo cambian sus resultados con respecto a la sección anterior? ¿A qué se puede deber esto?\n",
        "El mejor modelo obtenido con Optuna logró un MAE de 1999.37, mejorando significativamente el resultado anterior con XGBRegressor por defecto, que alcanzó un MAE de 2425.82. Esta mejora de aproximadamente 17.6% en el error absoluto medio sugiere que la configuración predeterminada del modelo no era óptima para este conjunto de datos. La optimización permitió encontrar una combinación más adecuada de parámetros que equilibra la profundidad, regularización, tasa de aprendizaje y complejidad del modelo, lo cual mejora su capacidad de generalizar patrones y ajustarse a las características específicas del problema. Además, ajustar min_frequency en OneHotEncoder ayudó a reducir ruido de categorías poco frecuentes, mejorando aún más la robustez del modelo.\n",
        "\n",
        "    Explique cada hiperparámetro y su rol en el modelo.\n",
        "* learning_rate: controla qué tan rápido aprende el modelo. Un valor moderado como 0.058 permite ajustes graduales, evitando sobreajuste. El rango (0.001, 0.1) es razonable para evitar valores extremos.\n",
        "* n_estimators: define la cantidad de árboles. Un valor alto como 999 es coherente con un learning_rate bajo, ya que se necesita más iteraciones para converger. El rango (50, 1000) es apropiado.\n",
        "* max_depth y max_leaves: controlan la complejidad del árbol. Valores altos como 9 y 96 permiten al modelo capturar interacciones complejas. Los rangos dados son lógicos para evitar tanto modelos muy simples como sobreajuste extremo.\n",
        "* min_child_weight: evita que nodos con poca información sean divididos. Un valor de 4 reduce el riesgo de overfitting. El rango (1, 5) es adecuado.\n",
        "* reg_alpha y reg_lambda: controlan la regularización L1 y L2 respectivamente. Ambos ayudan a prevenir el sobreajuste. Los valores encontrados (~0.03) sugieren que cierta penalización mejora la generalización. El rango (0, 1) es estándar y apropiado.\n",
        "* min_frequency en OneHotEncoder: filtra categorías poco frecuentes. Un valor bajo (0.02) permite mantener la mayor parte de la información sin introducir ruido. El rango (0.0, 1.0) ofrece flexibilidad para distintos niveles de cardinalidad.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    ¿Hacen sentido los rangos de optimización indicados?\n",
        "En conjunto, los rangos de optimización son adecuados, y los valores encontrados tienen sentido dado el problema y los resultados obtenidos."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5195ccfc37e044ad9453f6eb2754f631",
        "deepnote_cell_type": "markdown",
        "id": "ZglyD_QWI5wA"
      },
      "source": [
        "## 4. Optimización de Hiperparámetros con Optuna y Prunners (17 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.pinimg.com/originals/90/16/f9/9016f919c2259f3d0e8fe465049638a7.gif\">\n",
        "</p>\n",
        "\n",
        "Después de optimizar el rendimiento de su modelo varias veces, Fiu le pregunta si no es posible optimizar el entrenamiento del modelo en sí mismo. Después de leer un par de post de personas de dudosa reputación en la *deepweb*, usted llega a la conclusión que puede cumplir este objetivo mediante la implementación de **Prunning**.\n",
        "\n",
        "Vuelva a optimizar los mismos hiperparámetros que la sección pasada, pero esta vez utilizando **Prunning** en la optimización. En particular, usted debe:\n",
        "\n",
        "- Responder: ¿Qué es prunning? ¿De qué forma debería impactar en el entrenamiento? [2 puntos]\n",
        "- Redefinir la función `objective()` utilizando `optuna.integration.XGBoostPruningCallback` como método de **Prunning** [10 puntos]\n",
        "- Fijar nuevamente el tiempo de entrenamiento a 5 minutos [1 punto]\n",
        "- Reportar el número de *trials*, el `MAE` y los mejores hiperparámetros encontrados. ¿Cómo cambian sus resultados con respecto a la sección anterior? ¿A qué se puede deber esto? [3 puntos]\n",
        "- Guardar su modelo en un archivo .pkl [1 punto]\n",
        "\n",
        "Nota: Si quieren silenciar los prints obtenidos en el prunning, pueden hacerlo mediante el siguiente comando:\n",
        "\n",
        "```\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "```\n",
        "\n",
        "De implementar la opción anterior, pueden especificar `show_progress_bar = True` en el método `optimize` para *más sabor*.\n",
        "\n",
        "Hint: Si quieren especificar parámetros del método .fit() del modelo a través del pipeline, pueden hacerlo por medio de la siguiente sintaxis: `pipeline.fit(stepmodelo__parametro = valor)`\n",
        "\n",
        "Hint2: Este <a href = https://stackoverflow.com/questions/40329576/sklearn-pass-fit-parameters-to-xgboost-in-pipeline>enlace</a> les puede ser de ayuda en su implementación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IEEZnb-cwP_"
      },
      "outputs": [],
      "source": [
        "#!pip install optuna-integration[xgboost]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "eeaa967cd8f6426d8c54f276c17dce79",
        "deepnote_cell_type": "code",
        "id": "sST6Wtj5I5wA"
      },
      "outputs": [],
      "source": [
        "# Inserte su código acá"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8a081778cc704fc6bed05393a5419327",
        "deepnote_cell_type": "markdown",
        "id": "ZMiiVaCUI5wA"
      },
      "source": [
        "## 5. Visualizaciones (5 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/F-LgB1xTebEAAAAd/look-at-this-graph-nickelback.gif\">\n",
        "</p>\n",
        "\n",
        "\n",
        "Satisfecho con su trabajo, Fiu le pregunta si es posible generar visualizaciones que permitan entender el entrenamiento de su modelo.\n",
        "\n",
        "A partir del siguiente <a href = https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html#visualization>enlace</a>, genere las siguientes visualizaciones:\n",
        "\n",
        "1. Gráfico de historial de optimización [1 punto]\n",
        "2. Gráfico de coordenadas paralelas [1 punto]\n",
        "3. Gráfico de importancia de hiperparámetros [1 punto]\n",
        "\n",
        "Comente sus resultados:\n",
        "\n",
        "4. ¿Desde qué *trial* se empiezan a observar mejoras notables en sus resultados? [0.5 puntos]\n",
        "5. ¿Qué tendencias puede observar a partir del gráfico de coordenadas paralelas? [1 punto]\n",
        "6. ¿Cuáles son los hiperparámetros con mayor importancia para la optimización de su modelo? [0.5 puntos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "0e706dc9a8d946eda7a9eb1f0463c6d7",
        "deepnote_cell_type": "code",
        "id": "xjxAEENAI5wA"
      },
      "outputs": [],
      "source": [
        "# Inserte su código acá"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ac8a20f445d045a3becf1a518d410a7d",
        "deepnote_cell_type": "markdown",
        "id": "EoW32TA9I5wA"
      },
      "source": [
        "## 6. Síntesis de resultados (3 puntos)\n",
        "\n",
        "Finalmente:\n",
        "\n",
        "1. Genere una tabla resumen del MAE en el conjunto de validación obtenido en los 5 modelos entrenados desde Baseline hasta XGBoost con Constraints, Optuna y Prunning. [1 punto]\n",
        "2. Compare los resultados de la tabla y responda, ¿qué modelo obtiene el mejor rendimiento? [0.5 puntos]\n",
        "3. Cargue el mejor modelo, prediga sobre el conjunto de **test** y reporte su MAE. [0.5 puntos]\n",
        "4. ¿Existen diferencias con respecto a las métricas obtenidas en el conjunto de validación? ¿Porqué puede ocurrir esto? [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq5C6cDnJg9h"
      },
      "outputs": [],
      "source": [
        "# Inserte su código acá"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5c4654d12037494fbd385b4dc6bd1059",
        "deepnote_cell_type": "markdown",
        "id": "E_19tgBEI5wA"
      },
      "source": [
        "# Conclusión\n",
        "Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.pinimg.com/originals/55/3d/42/553d42bea9b10e0662a05aa8726fc7f4.gif\">\n",
        "</p>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5025de06759f4903a26916c80323bf25",
        "deepnote_cell_type": "markdown",
        "id": "Kq2cFix1I5wA"
      },
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "rAp9UxwiI5wA"
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "f63d38450a6b464c9bb6385cf11db4d9",
    "deepnote_persisted_session": {
      "createdAt": "2023-11-09T16:18:30.203Z"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
